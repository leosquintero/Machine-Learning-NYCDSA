# uploading data sets
train <- read.csv("./Data/train.csv", stringsAsFactors = FALSE)
train_original <- read.csv("./Data/train.csv", stringsAsFactors = FALSE)
#### wrangling and cleaning ####
# Taking a look at the head
head(train)
# taking a look at the dimentions
dim(train)
# taking a look at the structure
str(train)
# taking a look at the summary
summary(train)
colnames(train)
# Droping column ID
train <- train[, -1]
# finding how many Numerical variables there are in the data set
x <- dplyr::select_if(train, is.numeric)
# transforming numerical columns that should be considered categorical
train$MSSubClass = as.character(train$MSSubClass)
train$YearRemodAdd = as.character(train$YearRemodAdd)
# Generating a plot with all numerical values
x %>%
keep(is.numeric) %>%
gather() %>%
ggplot(aes(value))+
facet_wrap(~ key, scales = "free")+
geom_histogram()
#### Missing Data ####
# counting missing values
sum(is.na(train))
# Percentage of data missing in the data frame (0.05889565)
sum(is.na(train)) / (nrow(train) * ncol(train))
# counting missing values per column
data.frame(sapply(train, function(y) sum(length(which(is.na(y))))))
sum(train$Sal)
# plotting missing data and finding patterns
gg_miss_upset(train)
# Imputing missing data with the mean of each numerical column
for (i in 1:ncol(train)) {
train[is.na(train[, i]), i] <- mean(train[, i], na.rm = TRUE)
}
# Erasing columns with too many missing values
train <- train[, -which(names(train) %in% c(
"Alley", "PoolQC", "Fence",
"MiscFeature", "FireplaceQu"
))]
nrow(train)
# Deleting rows with remaining missing values
# train_complete <- train[complete.cases(train), ]
# nrow(train_complete)
# Counting missing values again
sum(is.na(train)) / (nrow(train) * ncol(train))
#New dimensions
dim(train)
#### Dummies ####
# dummifying Categorical(Factor) columns
t <- dummyVars("~.", data = train, drop2nd = TRUE)
train <- data.frame(predict(t, newdata = train))
sum(train$SalePrice != train_original$SalePrice)
#### Feature Selection ####
# creating new feature adding the total area of the house
train <- train %>%
mutate(
TotalSF = TotalBsmtSF + X1stFlrSF + X2ndFlrSF,
TotalBsmtSF = NULL, X1stFlrSF = NULL, X2ndFlrSF = NULL
)
train <- train[ , -which(names(train) %in% c("BsmtFinSF1","BsmtFinSF2", "BsmtUnfSF"))]
# Running a preliminar multiple linear model
# to evaluate the relevance of all variables
model<- lm(SalePrice ~ .,  data = train)
summary(model)
#Selecting all variables with P value < 0.04
tm <- tidy(model)
# visualise dataframe of the model
# (using non scientific notation of numbers)
options(scipen = 999)
train_rel <- tm$term[tm$p.value < 0.05]
#obtaining a data frame with the column names
# that match the previous condition
train_rel <- train %>%
select(train_rel)
# adding Saleprice to new data set
train_rel["SalePrice"] <- train$SalePrice
# running a nwe lm including only features with p-value smaller than 0.05
summary(lm(SalePrice ~ ., data = train_rel))
tm %>% arrange(tm$p.value)
library(ggplot2)
library(tidyverse)
library(dummies)
library(class)
library(corrplot)
library(caret)
library(broom)
library(purrr)
library(ggplot2)
library(gridExtra)
library(corrplot)
library(naniar)
##### Data Cleaning#####
# uploading data sets
train <- read.csv("./Data/train.csv", stringsAsFactors = FALSE)
train_original <- read.csv("./Data/train.csv", stringsAsFactors = FALSE)
#### wrangling and cleaning ####
# Taking a look at the head
head(train)
# taking a look at the dimentions
dim(train)
# taking a look at the structure
str(train)
# taking a look at the summary
summary(train)
colnames(train)
# Droping column ID
train <- train[, -1]
# finding how many Numerical variables there are in the data set
x <- dplyr::select_if(train, is.numeric)
# transforming numerical columns that should be considered categorical
train$MSSubClass = as.character(train$MSSubClass)
train$YearRemodAdd = as.character(train$YearRemodAdd)
# Generating a plot with all numerical values
x %>%
keep(is.numeric) %>%
gather() %>%
ggplot(aes(value))+
facet_wrap(~ key, scales = "free")+
geom_histogram()
#### Missing Data ####
# counting missing values
sum(is.na(train))
# Percentage of data missing in the data frame (0.05889565)
sum(is.na(train)) / (nrow(train) * ncol(train))
# counting missing values per column
data.frame(sapply(train, function(y) sum(length(which(is.na(y))))))
sum(train$Sal)
# plotting missing data and finding patterns
gg_miss_upset(train)
# Imputing missing data with the mean of each numerical column
for (i in 1:ncol(train)) {
train[is.na(train[, i]), i] <- mean(train[, i], na.rm = TRUE)
}
# Erasing columns with too many missing values
train <- train[, -which(names(train) %in% c(
"Alley", "PoolQC", "Fence",
"MiscFeature", "FireplaceQu"
))]
nrow(train)
# Deleting rows with remaining missing values
# train_complete <- train[complete.cases(train), ]
# nrow(train_complete)
# Counting missing values again
sum(is.na(train)) / (nrow(train) * ncol(train))
#New dimensions
dim(train)
#### Dummies ####
# dummifying Categorical(Factor) columns
t <- dummyVars("~.", data = train, drop2nd = TRUE)
train <- data.frame(predict(t, newdata = train))
sum(train$SalePrice != train_original$SalePrice)
#### Feature Selection ####
# creating new feature adding the total area of the house
train <- train %>%
mutate(
TotalSF = TotalBsmtSF + X1stFlrSF + X2ndFlrSF,
TotalBsmtSF = NULL, X1stFlrSF = NULL, X2ndFlrSF = NULL
)
train <- train[ , -which(names(train) %in% c("BsmtFinSF1","BsmtFinSF2", "BsmtUnfSF"))]
# Running a preliminar multiple linear model
# to evaluate the relevance of all variables
model<- lm(SalePrice ~ .,  data = train)
summary(model)
#Selecting all variables with P value < 0.04
tm <- tidy(model)
# visualise dataframe of the model
# (using non scientific notation of numbers)
options(scipen = 999)
train_rel <- tm$term[tm$p.value < 0.05]
#obtaining a data frame with the column names
# that match the previous condition
train_rel <- train %>%
select(train_rel)
# adding Saleprice to new data set
train_rel["SalePrice"] <- train$SalePrice
# running a nwe lm including only features with p-value smaller than 0.05
summary(lm(SalePrice ~ ., data = train_rel))
setwd("C:/Users/Usuario/Desktop/Machine-Learning-NYCDSA/Data")
library(ggplot2)
library(tidyverse)
library(dummies)
library(class)
library(corrplot)
library(caret)
library(broom)
library(purrr)
library(ggplot2)
library(gridExtra)
library(corrplot)
library(naniar)
##### Data Cleaning#####
# uploading data sets
train <- read.csv("./Data/train.csv", stringsAsFactors = FALSE)
train_original <- read.csv("./Data/train.csv", stringsAsFactors = FALSE)
#### wrangling and cleaning ####
# Taking a look at the head
head(train)
# taking a look at the dimentions
dim(train)
# taking a look at the structure
str(train)
# taking a look at the summary
summary(train)
colnames(train)
# Droping column ID
train <- train[, -1]
# finding how many Numerical variables there are in the data set
x <- dplyr::select_if(train, is.numeric)
# transforming numerical columns that should be considered categorical
train$MSSubClass = as.character(train$MSSubClass)
train$YearRemodAdd = as.character(train$YearRemodAdd)
# Generating a plot with all numerical values
x %>%
keep(is.numeric) %>%
gather() %>%
ggplot(aes(value))+
facet_wrap(~ key, scales = "free")+
geom_histogram()
#### Missing Data ####
# counting missing values
sum(is.na(train))
# Percentage of data missing in the data frame (0.05889565)
sum(is.na(train)) / (nrow(train) * ncol(train))
# counting missing values per column
data.frame(sapply(train, function(y) sum(length(which(is.na(y))))))
sum(train$Sal)
# plotting missing data and finding patterns
gg_miss_upset(train)
# Imputing missing data with the mean of each numerical column
for (i in 1:ncol(train)) {
train[is.na(train[, i]), i] <- mean(train[, i], na.rm = TRUE)
}
# Erasing columns with too many missing values
train <- train[, -which(names(train) %in% c(
"Alley", "PoolQC", "Fence",
"MiscFeature", "FireplaceQu"
))]
nrow(train)
# Deleting rows with remaining missing values
# train_complete <- train[complete.cases(train), ]
# nrow(train_complete)
# Counting missing values again
sum(is.na(train)) / (nrow(train) * ncol(train))
#New dimensions
dim(train)
#### Dummies ####
# dummifying Categorical(Factor) columns
t <- dummyVars("~.", data = train, drop2nd = TRUE)
train <- data.frame(predict(t, newdata = train))
sum(train$SalePrice != train_original$SalePrice)
#### Feature Selection ####
# creating new feature adding the total area of the house
train <- train %>%
mutate(
TotalSF = TotalBsmtSF + X1stFlrSF + X2ndFlrSF,
TotalBsmtSF = NULL, X1stFlrSF = NULL, X2ndFlrSF = NULL
)
train <- train[ , -which(names(train) %in% c("BsmtFinSF1","BsmtFinSF2", "BsmtUnfSF"))]
# Running a preliminar multiple linear model
# to evaluate the relevance of all variables
model<- lm(SalePrice ~ .,  data = train)
summary(model)
#Selecting all variables with P value < 0.04
tm <- tidy(model)
# visualise dataframe of the model
# (using non scientific notation of numbers)
options(scipen = 999)
train_rel <- tm$term[tm$p.value < 0.05]
#obtaining a data frame with the column names
# that match the previous condition
train_rel <- train %>%
select(train_rel)
# adding Saleprice to new data set
train_rel["SalePrice"] <- train$SalePrice
# running a nwe lm including only features with p-value smaller than 0.05
summary(lm(SalePrice ~ ., data = train_rel))
# adding Saleprice to new data set
train_rel["SalePrice"] <- train$SalePrice
# uploading data sets
train <- read.csv("./Data/train.csv", stringsAsFactors = FALSE)
train_original <- read.csv("./Data/train.csv", stringsAsFactors = FALSE)
setwd("C:/Users/Usuario/Desktop/Machine-Learning-NYCDSA")
# uploading data sets
train <- read.csv("./Data/train.csv", stringsAsFactors = FALSE)
# Droping column ID
train <- train[, -1]
head(train)
# taking a look at the dimentions
dim(train)
# taking a look at the structure
str(train)
# taking a look at the summary
summary(train)
colnames(train)
# Droping column ID
train <- train[, -1]
# finding how many Numerical variables there are in the data set
x <- dplyr::select_if(train, is.numeric)
# transforming numerical columns that should be considered categorical
train$MSSubClass = as.character(train$MSSubClass)
train$YearRemodAdd = as.character(train$YearRemodAdd)
# Generating a plot with all numerical values
x %>%
keep(is.numeric) %>%
gather() %>%
ggplot(aes(value))+
facet_wrap(~ key, scales = "free")+
geom_histogram()
#### Missing Data ####
# counting missing values
sum(is.na(train))
# Percentage of data missing in the data frame (0.05889565)
sum(is.na(train)) / (nrow(train) * ncol(train))
# counting missing values per column
data.frame(sapply(train, function(y) sum(length(which(is.na(y))))))
sum(train$Sal)
# plotting missing data and finding patterns
gg_miss_upset(train)
# Imputing missing data with the mean of each numerical column
for (i in 1:ncol(train)) {
train[is.na(train[, i]), i] <- mean(train[, i], na.rm = TRUE)
}
# Erasing columns with too many missing values
train <- train[, -which(names(train) %in% c(
"Alley", "PoolQC", "Fence",
"MiscFeature", "FireplaceQu"
))]
nrow(train)
# Deleting rows with remaining missing values
# train_complete <- train[complete.cases(train), ]
# nrow(train_complete)
# Counting missing values again
sum(is.na(train)) / (nrow(train) * ncol(train))
#New dimensions
dim(train)
#### Dummies ####
# dummifying Categorical(Factor) columns
t <- dummyVars("~.", data = train, drop2nd = TRUE)
train <- data.frame(predict(t, newdata = train))
sum(train$SalePrice != train_original$SalePrice)
#### Feature Selection ####
# creating new feature adding the total area of the house
train <- train %>%
mutate(
TotalSF = TotalBsmtSF + X1stFlrSF + X2ndFlrSF,
TotalBsmtSF = NULL, X1stFlrSF = NULL, X2ndFlrSF = NULL
)
train <- train[ , -which(names(train) %in% c("BsmtFinSF1","BsmtFinSF2", "BsmtUnfSF"))]
# Running a preliminar multiple linear model
# to evaluate the relevance of all variables
model<- lm(SalePrice ~ .,  data = train)
summary(model)
#Selecting all variables with P value < 0.04
tm <- tidy(model)
# visualise dataframe of the model
# (using non scientific notation of numbers)
options(scipen = 999)
train_rel <- tm$term[tm$p.value < 0.05]
#obtaining a data frame with the column names
# that match the previous condition
train_rel <- train %>%
select(train_rel)
# adding Saleprice to new data set
train_rel["SalePrice"] <- train$SalePrice
# running a nwe lm including only features with p-value smaller than 0.05
summary(lm(SalePrice ~ ., data = train_rel))
tm %>% arrange(tm$p.value)
# Top p values ####
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
wine = read.csv("https://s3.amazonaws.com/nycdsabt01/Wine+Quality.csv")
wine = read.csv("https://s3.amazonaws.com/nycdsabt01/Wine+Quality.csv")
head(wine)
ifelse(wine$quality <= 5, "Low", "High")
wine$quality <- ifelse(wine$quality <= 5, "Low", "High")
head(wine)
wine_scale = as.data.frame(scale(wine[, -12]))
wine = cbind(wine.scale, quality)
wine = cbind(wine_scale, quality)
quality <- ifelse(wine$quality <= 5, "Low", "High")
wine = read.csv("https://s3.amazonaws.com/nycdsabt01/Wine+Quality.csv")
quality <- ifelse(wine$quality <= 5, "Low", "High")
wine_scale = as.data.frame(scale(wine[, -12]))
wine = cbind(wine_scale, quality)
wine$quality <- ifelse(wine$quality <= 5, "Low", "High")
View(wine_scale)
View(wine_scale)
wine_scale = as.data.frame(scale(wine[, -13]))
library(dplyr)
wine_scale <- wine %>% select_if(in.numeric, scale)
wine_scale <- wine %>% mutate_if(in.numeric, scale)
wine_scale <- wine %>% mutate_if(is.numeric, scale)
wine = read.csv("https://s3.amazonaws.com/nycdsabt01/Wine+Quality.csv")
wine$quality <- ifelse(wine$quality <= 5, "Low", "High")
wine_scale <- wine %>% mutate_if(is.numeric, scale)
wine = cbind(wine_scale, quality)
View(wine)
wine = read.csv("https://s3.amazonaws.com/nycdsabt01/Wine+Quality.csv")
wine$quality <- ifelse(wine$quality <= 5, "Low", "High")
wine_scale <- wine %>% mutate_if(is.numeric, scale)
plot(wine)```
plot(wine$fixed.acidity)```
plot(wine$fixed.acidity)
barplot(wine$fixed.acidity)
plot(wine$fixed.acidity)
wine = read.csv("https://s3.amazonaws.com/nycdsabt01/Wine+Quality.csv")
plot(wine$fixed.acidity)
wine = read.csv("https://s3.amazonaws.com/nycdsabt01/Wine+Quality.csv")
wine.scale = as.data.frame(scale(wine[, -12]))
wine = cbind(wine.scale, quality)
plot(wine$fixed.acidity)
wine = read.csv("https://s3.amazonaws.com/nycdsabt01/Wine+Quality.csv")
wine$quality <- ifelse(wine$quality <= 5, "Low", "High")
wine_scale <- wine %>% mutate_if(is.numeric, scale)
wine = read.csv("https://s3.amazonaws.com/nycdsabt01/Wine+Quality.csv")
wine$quality <- ifelse(wine$quality <= 5, "Low", "High")
wine_scale <- wine %>% mutate_if(is.numeric, scale)
plot(wine$fixed.acidity)
plot(wine$chlorides)
set.seed(0)
trin_set <- sample(1:nrow(wine), 8*nrow(wine)/10)
wine_train <- wine[train_set, ]
train_set <- sample(1:nrow(wine), 8*nrow(wine)/10)
wine_train <- wine[train_set, ]
wine_test <- wine[-train_set, ]
set.seed(0)
train_set <- sample(1:nrow(wine), 8*nrow(wine)/10)
wine_train <- wine[train_set, ]
wine_test <- wine[-train_set, ]
train <- wine[train_set, ]
test <- wine[-train_set, ]
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
library(dplyr)
wine = read.csv("https://s3.amazonaws.com/nycdsabt01/Wine+Quality.csv")
wine$quality <- ifelse(wine$quality <= 5, "Low", "High")
wine_scale <- wine %>% mutate_if(is.numeric, scale)
plot(wine$chlorides)
set.seed(0)
train_set <- sample(1:nrow(wine), 8*nrow(wine)/10)
train <- wine[train_set, ]
test <- wine[-train_set, ]
plot(wine[, -12], col = wine$quality)
plot(wine[, -12])
plot(wine)
plot(wine[, -12])
set.seed(0)
cv_wine <- tune(svm,
quality ~ .,
data = wine.train,
kernel = "linear",
ranges = list(cost = 10^(seq(-5, -.5, length = 50))))
cv_wine <- tune(svm,
quality ~ .,
data = wine_train,
kernel = "linear",
ranges = list(cost = 10^(seq(-5, -.5, length = 50))))
library(e1071)
install.packages("e1071")
library(e1071)
set.seed(0)
cv_wine <- tune(svm,
quality ~ .,
data = wine_train,
kernel = "linear",
ranges = list(cost = 10^(seq(-5, -.5, length = 50))))
cv_wine
cv_wine <- tune(svm, quality ~ ., data = wine_train, kernel = "linear",
ranges = list(cost = 10^(seq(-5, -.5, length = 50))))
cv_wine
cv_wine <- tune(svm, -quality ~ ., data = wine_train, kernel = "linear",
ranges = list(cost = 10^(seq(-5, -.5, length = 50))))
cv_wine <- tune(svm, quality ~ ., data = wine_train, kernel = "linear",
ranges = list(cost = 10^(seq(-5, -.5, length = 50))))
plot(cv.wine.svc.linear$performances$cost,
cv.wine.svc.linear$performances$error,
xlab = "Cost",
ylab = "Error Rate",
type = "l")
install.packages("psych")
#######################
#####Tools for PCA#####
#######################
library(psych) #Library that contains helpful PCA functions, such as:
############################
#####Data for Example 1#####
############################
bodies = Harman23.cor$cov #Covariance matrix of 8 physical measurements on 305 girls.
bodies
#######################
#####Tools for PCA#####
#######################
library(psych) #Library that contains helpful PCA functions, such as:
############################
#####Data for Example 1#####
############################
bodies = Harman23.cor$cov #Covariance matrix of 8 physical measurements on 305 girls.
bodies
####################
#####Choosing K#####
####################
fa.parallel(bodies, #The data in question.
n.obs = 305, #Since we supplied a covaraince matrix, need to know n.
fa = "pc", #Display the eigenvalues for PCA.
n.iter = 100) #Number of simulated analyses to perform.
install.packages("Sleuth2")
