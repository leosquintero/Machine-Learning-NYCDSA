wine <- read.csv("Wine", header = TRUE, stringsAsFactors = TRUE)
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document', echo=TRUE)
wine <- read.csv("wine", header = TRUE, stringsAsFactors = TRUE)
wine <- read.csv("wine", header = TRUE, stringsAsFactors = TRUE)
wine <- read.csv("wine", stringsAsFactors = TRUE)
setwd("F:/Carpetas/R")
wine <- read.csv("wine", stringsAsFactors = TRUE)
wine <- read.csv("wine.csv", stringsAsFactors = TRUE)
summary(wine)
head(wine)
lm(Price ~ AGST + age + FrancePop, data = wine)
lm(Price ~ AGST + Age + FrancePop, data = wine)
model1 <- lm(Price ~ AGST + Age + FrancePop, data = wine)
summary(model1)
model4 <- lm(Price ~ AGST + HarvestRain + WinterRain + Age, data = wine)
summary(model4)
cor(wine$WinterRain, wine$Price)
cor(wine$Age, wine$FrancePop)
cor(wine)
model5 <- ml(Price ~ AGST + HarvestRain + WinterRain + Age, data = wine)
model5 <- lm(Price ~ AGST + HarvestRain + WinterRain + Age, data = wine)
summary(model5)
winetest <- dead.csv("wine_test.csv")
winetest <- read.csv("wine_test.csv")
setwd("F:/Carpetas/R")
winetest <- read.csv("wine_test.csv")
predictTest <- predict(model4, newdata=winetest)
predictTest
str(winetest)
1 - SSE/SST
SSE <- sum((w)inetest - predictTest)^2)
SST <- sum((winetest$price - mean(wine$Price))^2)
1 - SSE/SST
SSE <- sum((winetest$price - predictTest)^2)
SST <- sum((winetest$price - mean(wine$Price))^2)
1 - SSE/SST
SSE = sum((winetest$price - predictTest)^2)
SST = sum((winetest$price - mean(wine$Price))^2)
1 - SSE/SST
1 - SSE/SST
NBA <- read.csv("NBA_train.csv")
getwd()
NBA <- read.csv("NBA_train.csv")
str(NBA)
table(NBA)
table(NBA$w, NBA$Playoffs)
NBA <- read.csv("NBA_train.csv", stringsAsFactors = TRUE)
table(NBA$w, NBA$Playoffs)
str(NBA)
table(NBA$W , NBA$Playoffs)
NBA$PTSdiff <- NBA$PTS - NBA$oppPTS
plot(NBA$PTSdiff, NBA$W)
WinsReg <- lm(W ~ PTSdiff, data = NBA)
summary(WinsReg)
MBA_test <- read.csv("NBA_test.csv")
pointspredictions <- predict(PointsReg4, newdata = NBA_test)
pointspredictions <- predict(PointsReg, newdata = NBA_test)
# 0: Load the data in RStudio
# 0.1 wraping original file into a data frame
refine_original <- read.csv("refine_original.csv", stringsAsFactors = FALSE)
library(rmarkdown)
library(ggplot2)
library(plotly)
install.packages(c("backports", "broom", "callr", "commonmark", "dplyr", "effects", "haven", "httpuv", "httr", "hunspell", "jsonlite", "knitr", "lme4", "markdown", "openssl", "pillar", "processx", "ps", "psych", "r2d3", "Rcpp", "RcppEigen", "readr", "readxl", "rmarkdown", "shiny", "sparklyr", "spelling", "survey"))
install.packages(c("httpuv", "readr", "readxl"))
library("markdown", lib.loc="~/R/win-library/3.5")
library("rmarkdown", lib.loc="~/R/win-library/3.5")
install.packages(c("curl", "dbplyr", "dplyr"))
install.packages(c("assertthat", "backports", "BH", "broom", "callr", "caret", "caTools", "cli", "clipr", "colorspace", "data.table", "evaluate", "forcats", "forge", "fs", "glue", "gower", "gtable", "haven", "highr", "httpuv", "knitr", "later", "lava", "lazyeval", "lme4", "modelr", "openssl", "processx", "ps", "psych", "purrr", "R6", "Rcpp", "RCurl", "readr", "readxl", "recipes", "rgl", "rJava", "rmarkdown", "RMySQL", "rstudioapi", "rvest", "shiny", "sparklyr", "spelling", "stringi", "stringr", "survey", "tidyr", "tinytex", "xfun", "XML"))
library("markdown", lib.loc="~/R/win-library/3.5")
library("rmarkdown", lib.loc="~/R/win-library/3.5")
library("commonmark", lib.loc="~/R/win-library/3.5")
library(plyr)
install.packages('rmarkdown')
install.packages('knitr')
install.packages("knitr")
install.packages('data.table')
library(plyr)
library(dplyr)
library("markdown", lib.loc="~/R/win-library/3.5")
library("rmarkdown", lib.loc="~/R/win-library/3.5")
detach("package:dplyr", unload=TRUE)
library("plyr", lib.loc="~/R/win-library/3.5")
library("dplyr", lib.loc="~/R/win-library/3.5")
rmarkdown::render("example.Rmd")
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
library(dplyr)
library(ggplot2)
ggplot(data = diamonds, aes(x = color)) +
geom_bar(aes(fill = cut), position = "dodge")
raw.df <- fread("./data.csv", stringsAsFactors = F)
install.packages('VIM')
library(VIM)
help(sleep) #Inspecting the mammal sleep dataset.
install.packages("mice")
install.packages("caret")
install.packages("Hmisc")
source('C:/Users/Usuario/Desktop/NYCDSA/43 Simple lineal regression with R/SimpleLinearRegressionLecture.R', echo=TRUE)
cor(cars) #Correlations.
#Basic graphical EDA for cars dataset.
hist(cars$speed, xlab = "Speed in MPH", main = "Histogram of Speed")
hist(cars$dist, xlab = "Distance in Feet", main = "Histogram of Distance")
plot(cars, xlab = "Speed in MPH", ylab = "Distance in Feet",
main = "Scatterplot of Cars Dataset")
#Manual calculation of simple linear regression coefficients.
beta1 = sum((cars$speed - mean(cars$speed)) * (cars$dist - mean(cars$dist))) /
sum((cars$speed - mean(cars$speed))^2)
beta0 = mean(cars$dist) - beta1*mean(cars$speed)
#Adding the least squares regression line to the plot.
abline(beta0, beta1, lty = 2)
#Calculating the residual values.
residuals = cars$dist - (beta0 + beta1*cars$speed)
#Note the sum of the residuals is 0.
sum(residuals)
#Visualizing the residuals.
segments(cars$speed, cars$dist,
cars$speed, (beta0 + beta1*cars$speed),
col = "red")
text(cars$speed - .5, cars$dist, round(residuals, 2), cex = 0.5)
#################################################
#####Automatic example with the cars dataset#####
#################################################
model = lm(dist ~ speed, data = cars) #Use the linear model function lm() to
summary(model) #All the summary information for the model in question. Reports:
#Notice that the F-statistic value for the overall regression is the same as the
#square of the t-statistic value for the speed coefficient:
t.statistic = 9.464
f.statistic = 89.57
t.statistic^2
confint(model) #Creating 95% confidence intervals for the model coefficients.
####################################################
#####Checking assumptions with the cars dataset#####
####################################################
#Linearity
plot(cars, xlab = "Speed in MPH", ylab = "Distance in Feet",
main = "Scatterplot of Cars Dataset")
abline(model, lty = 2)
#Constant Variance & Independent Errors
plot(model$fitted, model$residuals,
xlab = "Fitted Values", ylab = "Residual Values",
main = "Residual Plot for Cars Dataset")
abline(h = 0, lty = 2)
#Normality
qqnorm(model$residuals)
qqline(model$residuals)
#Using the built-in plot() function to visualize the residual plots.
plot(model) #Note the addition of the loess smoother and scale-location plot
#####Predicting New Observations#####
#####################################
model$fitted.values #Returns the fitted values.
newdata = data.frame(speed = c(15, 20, 25)) #Creating a new data frame to pass
predict(model, newdata, interval = "confidence") #Construct confidence intervals
predict(model, newdata, interval = "prediction") #Construct prediction invervals
#Constructing confidence and prediction bands for the scope of our data.
newdata = data.frame(speed = 4:25)
conf.band = predict(model, newdata, interval = "confidence")
pred.band = predict(model, newdata, interval = "prediction")
#Visualizing the confidence and prediction bands.
plot(cars, xlab = "Speed in MPH", ylab = "Distance in Feet",
main = "Scatterplot of Cars Dataset")
#Visualizing the confidence and prediction bands.
plot(cars, xlab = "Speed in MPH", ylab = "Distance in Feet",
main = "Scatterplot of Cars Dataset")
abline(model, lty = 2) #Plotting the regression line.
lines(newdata$speed, conf.band[, 2], col = "blue") #Plotting the lower confidence band.
lines(newdata$speed, conf.band[, 3], col = "blue") #Plotting the upper confidence band.
lines(newdata$speed, pred.band[, 2], col = "red") #Plotting the lower prediction band.
lines(newdata$speed, pred.band[, 3], col = "red") #Plotting the upper prediction band.
legend("topleft", c("Regression Line", "Conf. Band", "Pred. Band"),
lty = c(2, 1, 1), col = c("black", "blue", "red"))
legend("topleft", c("Regression Line", "Conf. Band", "Pred. Band"),
lty = c(2, 1, 1), col = c("black", "blue", "red"))
####################################
#####The Box-Cox Transformation#####
####################################
library(car)
bc = boxCox(model) #Automatically plots a 95% confidence interval for the lambda
lambda = bc$x[which(bc$y == max(bc$y))] #Extracting the best lambda value.
dist.bc = (cars$dist^lambda - 1)/lambda #Applying the Box-Cox transformation.
model.bc = lm(dist.bc ~ cars$speed) #Creating a new regression based on the
install.packages("tree")
install.packages(c("gbm", "randomForest"))
##############################
#####Classification Trees#####
##############################
#Loading the tree library for fitting classification and regression trees.
library(tree)
install.packages("tree")
install.packages(c("dbplyr", "digest", "dplyr", "DT", "fs", "ggthemes", "googleVis", "gower", "hexbin", "ipred", "jomo", "knitr", "Lahman", "mice", "pillar", "processx", "progress", "raster", "rcmdcheck", "reprex", "rmarkdown", "robustbase", "rvest", "shiny", "sparklyr", "survey", "swirl", "sys", "testthat", "tinytex", "xfun", "xtable", "zip"))
#Loading the ISLR library in order to use the Carseats dataset.
library(ISLR)
install.packages("installr")
library(installr)
version
update
install.packages("tree")
install.packages("EBImage")
run knitr::stitch('Data Cleaning and EDA.R')
knitr::stitch('Data Cleaning and EDA.R')
setwd("C:/Users/Usuario/Desktop/Machine-Learning-NYCDSA/data cleaning")
knitr::stitch('Data Cleaning and EDA.R')
knitr::stitch('Data Cleaning and EDA.R')
install.packages('tinytex')
install.packages("tinytex")
library(tinytex)
knitr::stitch('Data Cleaning and EDA.R')
options(tinytex.verbose = TRUE)
knitr::stitch('Data Cleaning and EDA.R')
knitr::stitch('Data Cleaning and EDA.r')
knitr::stitch('Data Cleaning and EDA.R*')
knitr::stitch('Data Cleaning and EDA.R')
install.packages('tex')
install.packages('Latex')
