mutate(TotalSF = TotalBsmtSF + X1stFlrSF + X2ndFlrSF,
TotalBsmtSF = NULL, X1stFlrSF = NULL, X2ndFlrSF = NULL)
train <- train[ , -which(names(train) %in% c("BsmtFinSF1","BsmtFinSF2", "BsmtUnfSF"))]
# Running a preliminar multiple linear model
# to evaluate the relevance of all variables
model<- lm(SalePrice ~ .,  data = train)
summary(model)
#Selecting all variables with P value < 0.04
tm <- tidy(model)
# visualise dataframe of the model
# (using non scientific notation of numbers)
options(scipen = 999)
train_rel <- tm$term[tm$p.value < 0.05]
#obtaining a data frame with the column names
# that match the previous condition
train_rel <- train %>%
select(train_rel)
#adding Saleprice to new data set
train_rel["SalePrice"] <- train$SalePrice
# running a nwe lm including only features
# with p-value smaller than 0.05
summary(lm(SalePrice ~ .,  data = train_rel))
options(scipen = 999)
tm %>% arrange(tm$p.value)
# Top p values ####
setwd("C:/Users/Usuario/Desktop/Machine-Learning-NYCDSA/Data")
#uploading data sets
train = read.csv("train.csv", stringsAsFactors = FALSE)
# Droping column ID
train <- train[, -1]
x <- dplyr::select_if(train, is.numeric)
# transforming numerical columns that should be considered categorical
train$MSSubClass = as.character(train$MSSubClass)
train$YearRemodAdd = as.character(train$YearRemodAdd)
# Generating a plot with all numerical values
x %>%
keep(is.numeric) %>%
gather() %>%
ggplot(aes(value))+
facet_wrap(~ key, scales = "free")+
geom_histogram()
#### Missing Data ####
# counting missing values
sum(is.na(train))
# Percentage of data missing in the data frame (0.05889565)
sum(is.na(train)) / (nrow(train) *ncol(train))
# counting missing values per column
data.frame(sapply(train, function(y) sum(length(which(is.na(y))))))
# plotting missing data and finding patterns
gg_miss_upset(train)
# Imputing missing data with the mean of each numerical column
for(i in 1:ncol(train)){
train[is.na(train[,i]), i] <- mean(train[,i], na.rm = TRUE)
}
# Erasing columns with too many missing values
train <- train[ , -which(names(train) %in% c("Alley","PoolQC", "Fence",
"MiscFeature", "FireplaceQu"))]
# Deleting rows with remaining missing values
train <- train[complete.cases(train), ]
# Counting missing values again
sum(is.na(train)) / (nrow(train) *ncol(train))
#New dimensions
dim(train)
#### Dummies ####
# dummifying Categorical(Factor) columns
t <- dummyVars("~.", data = train, drop2nd = TRUE)
train <- data.frame(predict(t, newdata = train))
#### Feature Selection ####
#creating new feature adding the total area of the house
train <- train %>%
mutate(TotalSF = TotalBsmtSF + X1stFlrSF + X2ndFlrSF,
TotalBsmtSF = NULL, X1stFlrSF = NULL, X2ndFlrSF = NULL)
train <- train[ , -which(names(train) %in% c("BsmtFinSF1","BsmtFinSF2", "BsmtUnfSF"))]
# Running a preliminar multiple linear model
# to evaluate the relevance of all variables
model<- lm(SalePrice ~ .,  data = train)
summary(model)
#Selecting all variables with P value < 0.04
tm <- tidy(model)
# visualise dataframe of the model
# (using non scientific notation of numbers)
options(scipen = 999)
train_rel <- tm$term[tm$p.value < 0.05]
#obtaining a data frame with the column names
# that match the previous condition
train_rel <- train %>%
select(train_rel)
#adding Saleprice to new data set
train_rel["SalePrice"] <- train$SalePrice
# running a nwe lm including only features
# with p-value smaller than 0.05
summary(lm(SalePrice ~ .,  data = train_rel))
options(scipen = 999)
tm %>% arrange(tm$p.value)
# Top p values ####
# Deleting outliers
train_rel <- train_rel[-which(train_rel$LotArea > 30000),]
#log term of SalePrice sinse the data is skewed to the left
train_rel$SalePrice <- log(train_rel$SalePrice)
library(ggplot2)
library(tidyverse)
library(dummies)
library(class)
library(corrplot)
library(caret)
library(broom)
library(purrr)
library(ggplot2)
library(gridExtra)
library(corrplot)
library(naniar)
#uploading data sets
train = read.csv("train.csv", stringsAsFactors = FALSE)
#### Data cleaning ####
# Taking a look at the head
head(train)
# taking a look at the dimentions
dim(train)
#taking a look at the structure
str(train)
# taking a look at the summary
summary(train)
colnames(train)
# Droping column ID
train <- train[, -1]
# finding how many Numerical variables there are in the data set
x <- dplyr::select_if(train, is.numeric)
# transforming numerical columns that should be considered categorical
train$MSSubClass = as.character(train$MSSubClass)
train$YearRemodAdd = as.character(train$YearRemodAdd)
# Generating a plot with all numerical values
x %>%
keep(is.numeric) %>%
gather() %>%
ggplot(aes(value))+
facet_wrap(~ key, scales = "free")+
geom_histogram()
#### Missing Data ####
# counting missing values
sum(is.na(train))
# Percentage of data missing in the data frame (0.05889565)
sum(is.na(train)) / (nrow(train) *ncol(train))
# counting missing values per column
data.frame(sapply(train, function(y) sum(length(which(is.na(y))))))
# plotting missing data and finding patterns
gg_miss_upset(train)
# Imputing missing data with the mean of each numerical column
for(i in 1:ncol(train)){
train[is.na(train[,i]), i] <- mean(train[,i], na.rm = TRUE)
}
# Erasing columns with too many missing values
train <- train[ , -which(names(train) %in% c("Alley","PoolQC", "Fence",
"MiscFeature", "FireplaceQu"))]
# Deleting rows with remaining missing values
train <- train[complete.cases(train), ]
# Counting missing values again
sum(is.na(train)) / (nrow(train) *ncol(train))
#New dimensions
dim(train)
#### Dummies ####
# dummifying Categorical(Factor) columns
t <- dummyVars("~.", data = train, drop2nd = TRUE)
train <- data.frame(predict(t, newdata = train))
#### Feature Selection ####
#creating new feature adding the total area of the house
train <- train %>%
mutate(TotalSF = TotalBsmtSF + X1stFlrSF + X2ndFlrSF,
TotalBsmtSF = NULL, X1stFlrSF = NULL, X2ndFlrSF = NULL)
train <- train[ , -which(names(train) %in% c("BsmtFinSF1","BsmtFinSF2", "BsmtUnfSF"))]
# Running a preliminar multiple linear model
# to evaluate the relevance of all variables
model<- lm(SalePrice ~ .,  data = train)
summary(model)
#Selecting all variables with P value < 0.04
tm <- tidy(model)
# visualise dataframe of the model
# (using non scientific notation of numbers)
options(scipen = 999)
train_rel <- tm$term[tm$p.value < 0.05]
#obtaining a data frame with the column names
# that match the previous condition
train_rel <- train %>%
select(train_rel)
#adding Saleprice to new data set
train_rel["SalePrice"] <- train$SalePrice
# running a nwe lm including only features
# with p-value smaller than 0.05
summary(lm(SalePrice ~ .,  data = train_rel))
options(scipen = 999)
tm %>% arrange(tm$p.value)
# Top p values ####
# visualizing lineal model
model1 <- lm(SalePrice ~ .,  data = train_rel)
plot(model1)
# visualizing lineal model
model1 <- lm(SalePrice ~ .,  data = train_rel)
# running a nwe lm including only features
# with p-value smaller than 0.05
summary(lm(SalePrice ~ .,  data = train_rel))
tm %>% arrange(tm$p.value)
# visualizing lineal model
model1 <- lm(SalePrice ~ .,  data = train_rel)
library(ggplot2)
library(tidyverse)
library(dummies)
library(class)
library(corrplot)
library(caret)
library(broom)
library(purrr)
library(ggplot2)
library(gridExtra)
library(corrplot)
library(naniar)
#uploading data sets
train = read.csv("train.csv", stringsAsFactors = FALSE)
#### Data cleaning ####
# Taking a look at the head
head(train)
# taking a look at the dimentions
dim(train)
#taking a look at the structure
str(train)
# taking a look at the summary
summary(train)
colnames(train)
# Droping column ID
train <- train[, -1]
# finding how many Numerical variables there are in the data set
x <- dplyr::select_if(train, is.numeric)
# transforming numerical columns that should be considered categorical
train$MSSubClass = as.character(train$MSSubClass)
train$YearRemodAdd = as.character(train$YearRemodAdd)
# Generating a plot with all numerical values
x %>%
keep(is.numeric) %>%
gather() %>%
ggplot(aes(value))+
facet_wrap(~ key, scales = "free")+
geom_histogram()
#### Missing Data ####
# counting missing values
sum(is.na(train))
# Percentage of data missing in the data frame (0.05889565)
sum(is.na(train)) / (nrow(train) *ncol(train))
# counting missing values per column
data.frame(sapply(train, function(y) sum(length(which(is.na(y))))))
# plotting missing data and finding patterns
gg_miss_upset(train)
# Imputing missing data with the mean of each numerical column
for(i in 1:ncol(train)){
train[is.na(train[,i]), i] <- mean(train[,i], na.rm = TRUE)
}
# Erasing columns with too many missing values
train <- train[ , -which(names(train) %in% c("Alley","PoolQC", "Fence",
"MiscFeature", "FireplaceQu"))]
# Deleting rows with remaining missing values
train <- train[complete.cases(train), ]
# Counting missing values again
sum(is.na(train)) / (nrow(train) *ncol(train))
#New dimensions
dim(train)
#### Dummies ####
# dummifying Categorical(Factor) columns
t <- dummyVars("~.", data = train, drop2nd = TRUE)
train <- data.frame(predict(t, newdata = train))
#### Feature Selection ####
#creating new feature adding the total area of the house
train <- train %>%
mutate(TotalSF = TotalBsmtSF + X1stFlrSF + X2ndFlrSF,
TotalBsmtSF = NULL, X1stFlrSF = NULL, X2ndFlrSF = NULL)
train <- train[ , -which(names(train) %in% c("BsmtFinSF1","BsmtFinSF2", "BsmtUnfSF"))]
# Running a preliminar multiple linear model
# to evaluate the relevance of all variables
model<- lm(SalePrice ~ .,  data = train)
summary(model)
#Selecting all variables with P value < 0.04
tm <- tidy(model)
# visualise dataframe of the model
# (using non scientific notation of numbers)
options(scipen = 999)
train_rel <- tm$term[tm$p.value < 0.05]
#obtaining a data frame with the column names
# that match the previous condition
train_rel <- train %>%
select(train_rel)
#adding Saleprice to new data set
train_rel["SalePrice"] <- train$SalePrice
# running a nwe lm including only features
# with p-value smaller than 0.05
summary(lm(SalePrice ~ .,  data = train_rel))
tm %>% arrange(tm$p.value)
# Top p values ####
train_rel <- tm$term[tm$p.value < 0.05]
#obtaining a data frame with the column names
# that match the previous condition
train_rel <- train %>%
select(train_rel)
#adding Saleprice to new data set
train_rel["SalePrice"] <- train$SalePrice
# running a nwe lm including only features
# with p-value smaller than 0.05
summary(lm(SalePrice ~ .,  data = train_rel))
tm %>% arrange(tm$p.value)
# Running a preliminar multiple linear model
# to evaluate the relevance of all variables
model<- lm(SalePrice ~ .,  data = train)
summary(model)
#Selecting all variables with P value < 0.04
tm <- tidy(model)
# visualise dataframe of the model
# (using non scientific notation of numbers)
options(scipen = 999)
train_rel <- tm$term[tm$p.value < 0.05]
#obtaining a data frame with the column names
# that match the previous condition
train_rel <- train %>%
select(train_rel)
library(ggplot2)
library(tidyverse)
library(dummies)
library(class)
library(corrplot)
library(caret)
library(broom)
library(purrr)
library(ggplot2)
library(gridExtra)
library(corrplot)
library(naniar)
#uploading data sets
train = read.csv("train.csv", stringsAsFactors = FALSE)
#### Data cleaning ####
# Taking a look at the head
head(train)
# taking a look at the dimentions
dim(train)
#taking a look at the structure
str(train)
# taking a look at the summary
summary(train)
colnames(train)
# Droping column ID
train <- train[, -1]
# finding how many Numerical variables there are in the data set
x <- dplyr::select_if(train, is.numeric)
# transforming numerical columns that should be considered categorical
train$MSSubClass = as.character(train$MSSubClass)
train$YearRemodAdd = as.character(train$YearRemodAdd)
# Generating a plot with all numerical values
x %>%
keep(is.numeric) %>%
gather() %>%
ggplot(aes(value))+
facet_wrap(~ key, scales = "free")+
geom_histogram()
#### Missing Data ####
# counting missing values
sum(is.na(train))
# Percentage of data missing in the data frame (0.05889565)
sum(is.na(train)) / (nrow(train) *ncol(train))
# counting missing values per column
data.frame(sapply(train, function(y) sum(length(which(is.na(y))))))
# plotting missing data and finding patterns
gg_miss_upset(train)
# Imputing missing data with the mean of each numerical column
for(i in 1:ncol(train)){
train[is.na(train[,i]), i] <- mean(train[,i], na.rm = TRUE)
}
# Erasing columns with too many missing values
train <- train[ , -which(names(train) %in% c("Alley","PoolQC", "Fence",
"MiscFeature", "FireplaceQu"))]
# Deleting rows with remaining missing values
train <- train[complete.cases(train), ]
# Counting missing values again
sum(is.na(train)) / (nrow(train) *ncol(train))
#New dimensions
dim(train)
#### Dummies ####
# dummifying Categorical(Factor) columns
t <- dummyVars("~.", data = train, drop2nd = TRUE)
train <- data.frame(predict(t, newdata = train))
#### Feature Selection ####
#creating new feature adding the total area of the house
train <- train %>%
mutate(TotalSF = TotalBsmtSF + X1stFlrSF + X2ndFlrSF,
TotalBsmtSF = NULL, X1stFlrSF = NULL, X2ndFlrSF = NULL)
train <- train[ , -which(names(train) %in% c("BsmtFinSF1","BsmtFinSF2", "BsmtUnfSF"))]
# Running a preliminar multiple linear model
# to evaluate the relevance of all variables
model<- lm(SalePrice ~ .,  data = train)
summary(model)
#Selecting all variables with P value < 0.04
tm <- tidy(model)
# visualise dataframe of the model
# (using non scientific notation of numbers)
train_rel <- tm$term[tm$p.value < 0.05]
#obtaining a data frame with the column names
# that match the previous condition
train_rel <- train %>%
select(train_rel)
#adding Saleprice to new data set
train_rel["SalePrice"] <- train$SalePrice
# running a nwe lm including only features
# with p-value smaller than 0.05
summary(lm(SalePrice ~ .,  data = train_rel))
tm %>% arrange(tm$p.value)
# visualizing lineal model
model1 <- lm(SalePrice ~ .,  data = train_rel)
library(ggplot2)
library(tidyverse)
library(dummies)
library(class)
library(corrplot)
library(caret)
library(broom)
library(purrr)
library(ggplot2)
library(gridExtra)
library(corrplot)
library(naniar)
#uploading data sets
train = read.csv("train.csv", stringsAsFactors = FALSE)
#### Data cleaning ####
# Taking a look at the head
head(train)
# taking a look at the dimentions
dim(train)
#taking a look at the structure
str(train)
# taking a look at the summary
summary(train)
colnames(train)
# Droping column ID
train <- train[, -1]
# finding how many Numerical variables there are in the data set
x <- dplyr::select_if(train, is.numeric)
# transforming numerical columns that should be considered categorical
train$MSSubClass = as.character(train$MSSubClass)
train$YearRemodAdd = as.character(train$YearRemodAdd)
# Generating a plot with all numerical values
x %>%
keep(is.numeric) %>%
gather() %>%
ggplot(aes(value))+
facet_wrap(~ key, scales = "free")+
geom_histogram()
#### Missing Data ####
# counting missing values
sum(is.na(train))
# Percentage of data missing in the data frame (0.05889565)
sum(is.na(train)) / (nrow(train) *ncol(train))
# counting missing values per column
data.frame(sapply(train, function(y) sum(length(which(is.na(y))))))
# plotting missing data and finding patterns
gg_miss_upset(train)
# Imputing missing data with the mean of each numerical column
for(i in 1:ncol(train)){
train[is.na(train[,i]), i] <- mean(train[,i], na.rm = TRUE)
}
# Erasing columns with too many missing values
train <- train[ , -which(names(train) %in% c("Alley","PoolQC", "Fence",
"MiscFeature", "FireplaceQu"))]
# Deleting rows with remaining missing values
train <- train[complete.cases(train), ]
# Counting missing values again
sum(is.na(train)) / (nrow(train) *ncol(train))
#New dimensions
dim(train)
#### Dummies ####
# dummifying Categorical(Factor) columns
t <- dummyVars("~.", data = train, drop2nd = TRUE)
train <- data.frame(predict(t, newdata = train))
#### Feature Selection ####
#creating new feature adding the total area of the house
train <- train %>%
mutate(TotalSF = TotalBsmtSF + X1stFlrSF + X2ndFlrSF,
TotalBsmtSF = NULL, X1stFlrSF = NULL, X2ndFlrSF = NULL)
train <- train[ , -which(names(train) %in% c("BsmtFinSF1","BsmtFinSF2", "BsmtUnfSF"))]
# Running a preliminar multiple linear model
# to evaluate the relevance of all variables
model<- lm(SalePrice ~ .,  data = train)
summary(model)
#Selecting all variables with P value < 0.04
tm <- tidy(model)
# visualise dataframe of the model
# (using non scientific notation of numbers)
train_rel <- tm$term[tm$p.value < 0.05]
#obtaining a data frame with the column names
# that match the previous condition
train_rel <- train %>%
select(train_rel)
#adding Saleprice to new data set
train_rel["SalePrice"] <- train$SalePrice
# running a nwe lm including only features
# with p-value smaller than 0.05
summary(lm(SalePrice ~ .,  data = train_rel))
# Running a preliminar multiple linear model
# to evaluate the relevance of all variables
model<- lm(SalePrice ~ .,  data = train)
summary(model)
View(model)
#Selecting all variables with P value < 0.05
tm <- tidy(model)
View(tm)
train_rel <- tm$term[tm$p.value < 0.05]
#obtaining a data frame with the column names
# that match the previous condition
train_rel <- train %>%
select(train_rel)
#adding Saleprice to new data set
train_rel["SalePrice"] <- train$SalePrice
